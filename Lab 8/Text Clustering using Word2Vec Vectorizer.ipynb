{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79a1a90-76bb-479c-9137-0f05285217ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "Purity: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naufa\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans \n",
    "from gensim.models import Word2Vec \n",
    "from tabulate import tabulate \n",
    "from collections import Counter\n",
    "\n",
    "dataset = [\"I love playing football on the weekends\", \n",
    "           \"I enjoy hiking and camping in the mountains\", \n",
    "           \"I like to read books and watch movies\", \n",
    "           \"I prefer playing video games over sports\", \n",
    "           \"I love listening to music and going to concerts\"] \n",
    "\n",
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4) \n",
    "\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in word2vec_model.wv], axis=0) for doc in dataset]) \n",
    "\n",
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k) \n",
    "km.fit(X) \n",
    " \n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    " \n",
    "# Tabulate the document and predicted cluster \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfebb228-19df-4926-8d2a-942b0215c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naufa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naufa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document                                Preprocessed                          Predicted Cluster\n",
      "-----------------------------------------------  ----------------------------------  -------------------\n",
      "I love playing football on the weekends          love playing football weekend                         1\n",
      "I enjoy hiking and camping in the mountains      enjoy hiking camping mountain                         0\n",
      "I like to read books and watch movies            like read book watch movie                            1\n",
      "I prefer playing video games over sports         prefer playing video game sport                       1\n",
      "I love listening to music and going to concerts  love listening music going concert                    1\n",
      "Purity (estimated): 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naufa\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize  # More reliable than word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = wordpunct_tokenize(text)  # Replaces word_tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Preprocess dataset\n",
    "preprocessed_dataset = [preprocess(doc) for doc in dataset]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=preprocessed_dataset, vector_size=100, window=5, min_count=1, workers=4, seed=42)\n",
    "\n",
    "# Convert documents to vectors by averaging word embeddings\n",
    "def document_vector(doc):\n",
    "    vectors = [word2vec_model.wv[word] for word in doc if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "X = np.array([document_vector(doc) for doc in preprocessed_dataset])\n",
    "\n",
    "# Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display results\n",
    "table_data = [[\"Original Document\", \"Preprocessed\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[orig, ' '.join(prep), cluster] for orig, prep, cluster in zip(dataset, preprocessed_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity (estimated):\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6476efb-7551-4dfd-bc4c-56f19159faac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
